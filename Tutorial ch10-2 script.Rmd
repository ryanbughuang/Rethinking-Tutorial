---
title: "Tutorial CH10-2"
author: "Ryan Huang"
date: "8/31/2019"
output:
  html_document:
    df_print: paged
    toc: true
    toc_depth: 4
    number_sections: true
    toc_float: true
---
```{r, results='hide'}
library(rethinking)
library(rstan)
library(tidyverse)
library(gridExtra)
library(skimr) # for n_unique func
library(DMwR) # for unscale func
compare = rethinking::compare
```

# CH10-2

## Poisson Regression
Binomial GLMs are appropriate when the outcome is a count from zero to a known upper bound.

When a binomial distribution has small probability of an event p and a large number of trials N, then it turns out to be a special shape, Poisson distribution.

To build a GLM with this likelihood, we need a link function. The conventional choice is the log link because we want to ensure that lambda is always positive.

$y_i \sim Poisson(\lambda_i)$
$log(\lambda_i) = \alpha_i + \beta_i log(P_i) $
$ alpha \sim Normal()$

### Data
The data we’ll work with are counts of unique tool types for 10 historical Oceanic societies

Assumptions: 

1. The number of tools increases with the log population size
2. The number of tools increases with the contact rate among islands
3. The impact of population on tool counts is increased by high contact(interaction)

```{r}
data(Kline)
d_Kline = Kline
d_Kline = d_Kline %>% 
  mutate(population_std = scale(log(population))[,1],
         contact_id = contact %>% as.integer()) # high = 1, low = 2

test_Kline = data.frame(
  pop_std = c(-5,seq(-1.5, 2.5, length.out = 99)),
  pop = c(-5,seq(-1.5, 2.5, length.out = 99)) %>% 
             unscale(., scale(log(d_Kline$population))) %>% 
             .[,1] %>% exp() %>% 
             rep(., 2),
  contact_id = c(rep(1, 100), rep(2, 100)),
  contact = c(rep("high", 100),rep("low", 100))
)
```
### Prior
A log link transforms the negative numbers to th range of 0 and 1 on the outcome scale.

If use a *flat* normal prior on alpha, the output lambda will concentrate between 0 and 1 while having a very long tail on the right.

A better prior will be normal(3, .5) which has a mean of 20 at the outcome scale and is relatively flat at the outcome scale.
```{r}
a1 = rnorm(1e4, 0, 10)
lambda1 = exp(a1)

a2 = rnorm(1e4, 3, .5)
lambda2 = exp(a2)


ggplot() +
  geom_density(aes(lambda1), adj=1e-10) +
  geom_density(aes(lambda2), adj=1e-10, color="blue") +
  xlim(0, 1e2)
```

For prior on beta, the author suggests to use normal(0, 0.2). 
Let's plot the full prior prediction.
$ \alpha \sim normal (3, 0.5)$
$ \beta \sim normal (0, 0.2)$

We want to have a regularized prior, so that most of our prior prediction should be horizental lines suggesting weak relationship between the predictor and the output.

```{r, fig.height=8, fig.width=6}
library(DMwR)
n = 100
pop_std = seq(from=-1.5, to=2.5, length.out = 100)
prior_a = rnorm(n, 3, 0.5)
prior_b = rnorm(n, 0, 0.2)

prior_pred_1 = ggplot() + xlab("log(x) scale")
prior_pred_2 = ggplot() + xlab("original x scale")
for(i in 1:n){
  result = data.frame(
    pop_std = pop_std,
    pred_Q    = exp(prior_a[i] + prior_b[i] * pop_std))
    
  prior_pred_1 = prior_pred_1 +
    geom_line(data = result, aes(pop_std, pred_Q))
  
  prior_pred_2 = prior_pred_2 +
    geom_line(data = result, aes(pop_std %>% 
                                 unscale(., scale(log(d_Kline$population))) %>%
                                 .[,1] %>% exp,
                                 pred_Q))
}

grid.arrange(prior_pred_1, prior_pred_2)
```

### Model

#### Math Model
```{r, results='hide'}
m10.3.1 = "
data {
	int N;
	int L;
	int tool[N];
	real population[N];
	int contact[N];
	int test_N;
	real test_pop[test_N];
	int test_contact[test_N];
}
parameters {
	real alpha[L];
	real beta[L];
}
transformed parameters {
	real lambda[N];
	for (i in 1:N){
		lambda[i] = exp(alpha[contact[i]] + beta[contact[i]] * population[i]);
	}
}
model {
	// model
	tool ~ poisson(lambda);

	// prior
	alpha ~ normal(3, 0.5);
	beta ~ normal(0, 0.2);
}
generated quantities {
	real log_lik[N];
	real pred_lambda[test_N];


	for (i in 1:N){
		log_lik[i] = poisson_lpmf(tool[i] | lambda[i]);
	}

	for (i in 1:test_N){
		pred_lambda[i] = exp(alpha[test_contact[i]] + beta[test_contact[i]] * test_pop[i]);
	}
}
"
dat10.3.1 = list(N = nrow(d_Kline),
                 L = d_Kline$contact %>% n_unique(),
                 tool = d_Kline$total_tools,
                 population = d_Kline$population_std,
                 contact = d_Kline$contact_id,
                 test_N = nrow(test_Kline),
                 test_pop = test_Kline$pop_std,
                 test_contact = test_Kline$contact_id %>% as.integer())

fit10.3.1 = stan(model_code = m10.3.1, data = dat10.3.1, cores = 2, chains = 2)
```
```{r}
print(fit10.3.1, pars = c("alpha", "beta"))
```

#### Scientific Model
* Tools are developed over time so:
  + Tool innovation is proportional to population size with diminishing returns 
  + Tool loss is proportional to the number of exsisting tools, with no diminishing returns.
  
$lambda = \alpha * P  ^\beta / \gamma $

```{r, results='hide'}
m10.3.2 = "
data {
	int N;
	int L;
	int tool[N];
	int population[N];
	int contact[N];

	int test_N;
	real test_population[test_N];
	int test_contact[test_N];
}
parameters {
	real<lower=0> alpha[L];
	real<lower=0> beta[L];
	real<lower=0> gamma;

}
transformed parameters {
	real lambda[N];
	for (i in 1:N){
		lambda[i] = alpha[contact[i]] * population[i] ^ beta[contact[i]] / gamma;
	}
}
model {
	// model
	tool ~ poisson(lambda);

	// prior
	alpha ~ lognormal(0, 1); // exp(normal(1,1)) = lognormal(0,1)
	beta ~ exponential(1);
	gamma ~ exponential(1);
}
generated quantities {
	real log_lik[N];
	real pred_lambda[test_N];

	for (i in 1:N){
		log_lik[i] = poisson_lpmf(tool[i] | lambda[i]);
	}

	for (i in 1:test_N){
		pred_lambda[i] = alpha[test_contact[i]] * test_population[i] ^ beta[test_contact[i]] / gamma;
	}
}
"

dat10.3.2 = list(
  N = nrow(d_Kline),
  L = d_Kline$contact %>% n_unique(),
  population = d_Kline$population,
  contact = d_Kline$contact_id,
  tool = d_Kline$total_tools,
  test_N = nrow(test_Kline),
  test_population = test_Kline$pop,
  test_contact = test_Kline$contact_id %>% as.integer()
)

fit10.3.2 = stan(model_code = m10.3.2, 
                 data = dat10.3.2, 
                 cores = 2, chains = 2, 
                 warmup = 1500, iter = 3000)
```
```{r}
print(fit10.3.2, pars = c("alpha", "beta", "gamma"))
```

### Posterior Prediction
#### Math Model
The trend for societies with high contact is higher than that for societies with low contact when population size is low, but then the model allows the trend for high contact to be smaller when population size is high.

Also, the math model has no guarantee that the trend for λ will pass through the origin where total tools equals zero and the population size equals zero.
```{r}
post10.3.1 = as.data.frame(fit10.3.1, pars = "pred_lambda")
result10.3.1 = test_Kline %>% 
  mutate(
    pred_tool = post10.3.1 %>% apply(., 2, mean),
    PI_lower = post10.3.1 %>% apply(., 2, HPDI) %>% .[1,],
    PI_upper = post10.3.1 %>% apply(., 2, HPDI) %>% .[2,])
p10.3.1 = result10.3.1 %>% 
  ggplot(aes(fill=contact)) +
  geom_line(aes(pop, pred_tool, color=contact)) +
  geom_ribbon(aes(x = pop, ymin=PI_lower, ymax=PI_upper), alpha = .3) +
  geom_point(data = d_Kline, aes(d_Kline$population, d_Kline$total_tools, fill=d_Kline$contact), shape=21)

```

#### Scientific Model
```{r}
post10.3.2 = as.data.frame(fit10.3.2, pars = "pred_lambda")
result10.3.2 = test_Kline %>% 
  mutate(
    pred_tool = post10.3.2 %>% apply(., 2, mean),
    PI_lower = post10.3.2 %>% apply(., 2, HPDI) %>% .[1,],
    PI_upper = post10.3.2 %>% apply(., 2, HPDI) %>% .[2,])
p10.3.2 = result10.3.2 %>% 
  ggplot(aes(fill=contact)) +
  geom_line(aes(pop, pred_tool, color=contact)) +
  geom_ribbon(aes(x = pop, ymin=PI_lower, ymax=PI_upper), alpha = .3) +
  geom_point(data = d_Kline, aes(d_Kline$population, d_Kline$total_tools, fill=d_Kline$contact), shape=21)

```

#### Model comparison
```{r, fig.height=6, fig.width=6}
grid.arrange(p10.3.1, p10.3.2, nrow=2)
```
```{r}
compare(fit10.3.1, fit10.3.2)
```


## Special Poisson Model
The parameter λ is the expected mean of a Poisson model, but it can also be thought of as a rate.

λ can be interpreted as how often an event will occur $\pi$ given a certain time period $\tau$.

$\lambda = \pi / \tau$

Here we will learn about modeling a poisson output with different time period.
For example, we have the *daily* record of the occurence of an event in place A. Also, we have the *weekly* record of the same event in place B. How can we compare these two records?

### Data
Let's create the fake data. Assuming that the ouuurence rates are 1.5/day in place A and 0.5/day in place B.

We want to estimate these two ouuurence rates using one model.
```{r}
num_days = 30
y_daily = rpois( num_days , 1.5 )
num_weeks = 4
y_weekly = rpois( num_weeks , 0.5*7 ) # 0.5/day -> 3.5/week
y_all = c( y_daily , y_weekly )

d = data.frame( 
  y = y_all , 
  days = c( rep(1,30) , rep(7,4) ), 
  freq = c( rep("daily",30) , rep("weekly",4) ) )

```

### Model

$\lambda_i = \pi_i / \tau_i$
$log(\lambda_i) = \alpha_i$
$log(\lambda_i) = log(\frac{\pi_i}{\tau_i}) = log(\pi_i) - log(\tau_i) = \alpha_i$
$log(\pi_i) = log(\tau_i) + \alpha_i$

```{r, results = "hide"}
m10.4.1 = "
data{
    int<lower=1> N;
    int<lower=1> L;
    int y[N];
    real log_days[N];
    int freq[N];
}
parameters{
    vector[L] a;
}
transformed parameters {
    vector[N] lambda;
    for ( i in 1:N ) {
        lambda[i] = exp(log_days[i] + a[freq[i]]);
    }
}
model{
    a ~ normal( 0 , 1 );
    y ~ poisson( lambda );
}
"
dat10.4.1 = list(N = nrow(d),
                 L = d$freq %>% n_unique(),
                 y = d$y, 
                 log_days = d$days %>% log, 
                 freq = d$freq %>% as.integer() )

fit10.4.1 = stan(model_code = m10.4.1, data = dat10.4.1)
```

```{r}
post10.4.1 = as.data.frame(fit10.4.1, pars = "a") %>% exp()
summary(post10.4.1)[4,]
```
After taking exponential on a, we get the posterior prediction of the two lambdas, the result shows that the approximation we have is close to the true value.

## Exponential Model: Survival Analysis
Poisson distribution describe the count of the occurence of an event during a time period. Similarly, we may also want to know the *time difference* between two occurences of an event. This process is described by exponential distribution.

### Data
Let's look at the cat adoption data. There are two groups of cats in the data, being adopted or not. 

We are wondering if the color of the cat has impact on the adoptation rate.
```{r, results='hide'}
# data from https://zenodo.org/record/2592740
d_cat = read_csv2("AustinCats.csv")
d_cat$adopt = ifelse( d_cat$out_event=="Adoption" , 1L , 0L )
d_cat$color_id = ifelse( d_cat$color=="Black" , 1L , 2L )
```

### Model

ref: <https://mc-stan.org/docs/2_18/functions-reference/exponential-distribution.html>

The adoption group follows a exponential distribution. 

For the other group, we can imagine that after 30 days half of the cats are adopted, then the probability of waiting 30 days and still not being adopted is 0.5.

Any rate of adoption implies a proportion of the cats that will remain after any given number of days.

So **one minus the cumulative distribution(1-cdf)**, or **complemantary cumulative distribution(ccdf)**,  gives the probability a cat is not adopted by the same number of days.

$Cats | adopted \sim exponential(\lambda_i D_i)$
$Cats | other \sim 1 - exponential_{lcdf}(\lambda_i D_i)$
$\lambda_i = 1.0 / \mu_i$ property of exp. distribution
$\mu_i \sim exp(\alpha_{color[i]})$

```{r, results='hide'}
m10.5.1 = "
data {
	int N;
	int L;
	int adopted[N];
	int color[N];
	int days_to_event[N];
}

parameters{
	real alpha[L];
}

transformed parameters {
	real lambda[N];
	real mu[N];
	for (i in 1:N){
		mu[i] = exp(alpha[color[i]]);
		lambda[i] = 1.0 / mu[i];
	}
}

model{
	
	// prior
	alpha ~ normal( 0 , 1 );
	
	// model
	for ( i in 1:N ){
		// adopted group
		if (adopted[i] == 1) {
			days_to_event[i] ~ exponential( lambda[i] );
			// target += exponential_lpdf(days_to_event[i] | lambda[i]);
		}

		// other group
		if ( adopted[i] == 0 ) {
    		target += exponential_lccdf(days_to_event[i] | lambda[i]);
		}
	}
}

generated quantities {
  vector[100] prop_left_black;
  vector[100] prop_left_other;

  for (i in 1:100){
    prop_left_black[i] = 1 - exponential_cdf( i,  1 / exp(alpha[1]));
    prop_left_other[i] = 1 - exponential_cdf( i,  1 / exp(alpha[2]));
  }
}
"

dat10.5.1 = list(
  N = d_cat %>% nrow,
  L = d_cat$color_id %>% n_unique(),
  color = d_cat$color_id,
  adopted = d_cat$adopt,
  days_to_event = d_cat$days_to_event
)

fit10.5.1 = stan(model_code = m10.5.1, data = dat10.5.1, cores = 2, chains = 2)
```
```{r}
print(fit10.5.1, pars = c("alpha"))
```

### Posterior
At the end, let's plot the predicted proportion left of cats in different colors. In the *generated quantities* block, I calculate the **propportion left** of cats in different colors from day 1 to day 100.

We can plot the result using similar codes as before.
```{r}
prop_left_black = as.data.frame(fit10.5.1, pars=c("prop_left_black"))
prop_left_other = as.data.frame(fit10.5.1, pars=c("prop_left_other"))

prop_left_black = data.frame(
  prop = prop_left_black %>% apply(., 2, mean),
  prop_lower = prop_left_black %>% apply(., 2, HPDI) %>% .[1,],
  prop_upper = prop_left_black %>% apply(., 2, HPDI) %>% .[2,]
)

prop_left_other = data.frame(
  prop = prop_left_other %>% apply(., 2, mean),
  prop_lower = prop_left_other %>% apply(., 2, HPDI) %>% .[1,],
  prop_upper = prop_left_other %>% apply(., 2, HPDI) %>% .[2,]
)

ggplot() +
  geom_line(data = prop_left_black, 
            aes(seq(1:100), prop), 
            color="black") +
  
  geom_ribbon(data = prop_left_black,
              aes(x=seq(1:100), 
                  ymin=prop_lower,
                  ymax=prop_upper), 
              fill = "black", alpha = .3) +
  
  geom_line(data = prop_left_other, 
            aes(seq(1:100), prop), 
            color="blue") +
  
  geom_ribbon(data = prop_left_other,
              aes(x=seq(1:100), 
                  ymin=prop_lower,
                  ymax=prop_upper), 
              fill = "blue", alpha = .3) +
  ylim(0, 1) +
  labs(x = "Days", y = "Proportion Remaining") +
  annotate("text", x=75, y=0.4, label= "Black Cats", color="black") + 
  annotate("text", x=40, y=.25, label= "Other Cats", color="blue")
```