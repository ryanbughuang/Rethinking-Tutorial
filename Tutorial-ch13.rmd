---
title: "Tutorial ch13 Advance Covariance"
author: "Ryan Huang"
date: "1/10/2020"
output:
  html_document:
    df_print: paged
    toc: true
    toc_depth: 4
    number_sections: true
    toc_float: true
---

```{r, results='hide'}
library(rethinking)
library(rstan)
library(tidyverse)
library(gridExtra)
library(skimr) # for n_unique func
library(DMwR) # for unscale func
library(MASS)
compare = rethinking::compare
select = dplyr::select
```

#Intro
**Ch13 in ver.1, Ch14 in ver.2**
This is the first chapter in the course about multivariate model, which means we will analyze more than one outcome variable at a time.

The concepts and codes are much more complicated than any other chapter in the textbook. Honestly I didn't understand most of the content in the class. I hope my learning experience can help you understand the material. Read the textbook may help you understand them better.

In previous chapter, we learn about the construction of **varying intercepts** model. In these models, there is only one parameters in each group. In this chapter, we will learn about the how to **pool information between parameters** when there are more than one parameters(ex: intercepts and slopes) in one group.

We will talk about these cases in the tutorial:

  1. more than one intercept(categorical variables, ver1-ch13.2)
  
  2. more than one intercept in many clusters(categorical variables, ver2-ch14.2)
  
  3. intercepts + slopes(categorical variable + continuous variables(ch14.4)
  
  4. gaussian process(continous categories, ver2-ch14.5)


#Case1: more than one intercept
Suppose we have a record of the waiting time of two cafés. The cafés vary in their average wait times($\alpha_{cafe[i]}$), and in their differences between morning and afternoon($\beta_{cafe[i]}$). There are now 2 parameters we are interested in now.

In tradictional approach, we will assign different prior for the two parameters, assuming they are indepent from others. But the fact is that they are probably related. At a popular café, wait times are on average long in the morning. But the same café will be much less busy in the afternoon since they have more staffs and fewer customers. But at a less popular café, the difference will be small. 

To pool the information from 2 parameters, we have to model their joint population, which means by modeling their covariance. The device that makes this possible is a **multivariate Gaussian distribution** for all of the varying effects.

In the first example, we will generate the fake café data and do the retrodiction to understand how the model works.

##Set hyper pars
Firstly we have to set the parameters for all cafes. 
```{r}
a = 3.5         #average morning wait time
b = (-1)        #average difference
sigma_a = 1     #std dev in wait time
sigma_b = .5    #std dev in difference
rho = (-0.7)    # correlation between a and b
```

Since we need to estimate 2 parameters at a time, we will need a 2-dimensional multivariate gaussian distribution. A 2-D multivariate gaussian is consisted of a 2x1 Mu vector and a 2x2 Sigma matrix.

$Mu = \left\{\begin{matrix} avg \ par1 \\ avg \ par2\end{matrix}\right\} \\ Sigma=\left\{\begin{matrix}var \ of \ par1 & cov \ of \ par1 \ and \ par 2  \\cov \ of \ par1 \ and \ par 2 & var \ of \ par2 \end{matrix} \right\}$

or in math form:

$Mu = \left\{\begin{matrix} \bar\alpha \\ \bar\beta\end{matrix}\right\} \\ Sigma=\left\{\begin{matrix}\sigma^2_{\alpha} & \sigma_{\alpha}\sigma_{\beta}\rho \\  \sigma_{\alpha}\sigma_{\beta}\rho & \sigma^2_{\beta} \end{matrix} \right\}$

Though we can build the Sigma matrix simply by putting the 4 elements all together, the recommended method is to do matrix multiplication:

$Sigma=\left\{\begin{matrix}\sigma^2_{\alpha} & \sigma_{\alpha}\sigma_{\beta}\rho \\  \sigma_{\alpha}\sigma_{\beta}\rho & \sigma^2_{\beta} \end{matrix} \right\} = \left\{\begin{matrix}\sigma_{\alpha} & 0 \\  0 & \sigma_{\beta} \end{matrix} \right\} \times \left\{\begin{matrix}1 & \rho \\  \rho & 1 \end{matrix} \right\} \times \left\{\begin{matrix}\sigma_{\alpha} & 0 \\  0 & \sigma_{\beta} \end{matrix} \right\}$


```{r}
Mu = c( a , b )
cov_ab = sigma_a*sigma_b*rho

#Method 1:
Sigma = matrix( c(sigma_a^2,cov_ab,cov_ab,sigma_b^2) ) 

#Method 2(recommend):
sigmas = c(sigma_a,sigma_b)
Rho = matrix( c(1,rho,rho,1) , nrow=2 )
Sigma = diag(sigmas) %*% Rho %*% diag(sigmas)
```

##cafe pars
Now we’re ready to simulate some cafés from the hyper parameters we have set. The results are the **average wait time** and **average difference** of each cafe.
```{r}
N_cafes = 20
vary_effects = mvrnorm( N_cafes , Mu , Sigma )

vary_effects = data.frame(
  a_cafe = vary_effects[,1],
  b_cafe = vary_effects[,2]
)

vary_effects
```
The result is a 20x2 matrix. Each row is a cafe while the first column is the wait time and the second column is the difference.

Let's visualize the result:
```{r}
p1.1 = vary_effects %>% 
  ggplot(aes(a_cafe, b_cafe)) +
  geom_point()

for (i in c(c(0.1,0.3,0.5,0.8,0.99))) {
  p1.1 = p1.1 + 
    stat_ellipse(type = "norm", level = i, alpha=.2)
}

p1.1
```
We can use the `stat_ellipse` function to plot a 2-D gaussian distribution. By setting different `level`, the function will plot the ellipses with different confidence intervals.

##Draw observation
The last step is to draw observations from the **average wait time** and **average difference** of each cafe. We will *visit* each cafe by 10 times, 5 in the morning and 5 in the afternoon. The resulting d_cafe dataframe will have 200 rows with 100 rows in the morning.
```{r}
N_visits = 10
sigma = 0.5 # std dev within cafes

d_cafe = data.frame(
  cafe_id = rep( 1:N_cafes , each=N_visits ),
  afternoon = rep(0:1,N_visits*N_cafes/2) # 0 = morning, 1 = afternoon
) %>% 
  mutate( 
    mu = vary_effects$a_cafe[cafe_id] + vary_effects$b_cafe[cafe_id]*afternoon,
    wait = rnorm( N_visits*N_cafes , mu , sigma ))

d_cafe = d_cafe %>% select(-mu)
```
##Model
Before getting into the stan model, let's briefly review what we have done in a math form:

$W_i \sim Normal(\mu_i, \sigma) \\ \mu_i = \alpha_{cafe[i]} + \beta_{cafe[i]}A_i \\ \begin{bmatrix}\alpha_{cafe} \\ \beta_{cafe} \end{bmatrix} \sim MVN(\begin{bmatrix}\alpha \\ \beta \end{bmatrix}, S) \\ S = \left\{\begin{matrix}\sigma_{\alpha} & 0 \\  0 & \sigma_{\beta} \end{matrix} \right\} \times \left\{\begin{matrix}1 & \rho \\  \rho & 1 \end{matrix} \right\} \times \left\{\begin{matrix}\sigma_{\alpha} & 0 \\  0 & \sigma_{\beta} \end{matrix} \right\}$

This line states that each café has 2 parameters with a prior distribution defined by the 2-D Gaussian distribution with *global* means $\alpha$ and $\beta$ and covariance matrix S.

$\begin{bmatrix}\alpha_{cafe} \\ \beta_{cafe} \end{bmatrix} \sim MVN(\begin{bmatrix}\alpha \\ \beta \end{bmatrix}, S)$

With this in mind, we are one step from building the stan model, how to set the prior. The prior for $\alpha, \beta, \sigma_{alpha}, \sigma_{\beta}$ are the same as previous models. The only trick is about setting the prior for the correlation matrix $R = \left\{\begin{matrix}1 & \rho \\  \rho & 1 \end{matrix} \right\}$. It isn’t easy to imagine what a distribution of
matrices means. But in this case, the correlation
matrix is only 2-by-2 so there is only one parameter $\rho$. For more complicated cases, the same prior, **LKJ distribution** still works.

Let's see how a LKJ distribution works.  LKJcorr(eta = 2) defines a weakly informative prior on ρ that is skeptical of extreme correlations near −1 or 1. LKJ distribution has a single parameter, eta. When we eta = 1, the prior is flat. When eta becomes larger, it will be more skeptical on the extreme values(-1 or 1).

###lkj prior
```{r}
?rlkjcorr
R = rlkjcorr(n = 1000, K = 3, eta = 2)  # K: dim, eta: shape

dim(R)

R[1,,]

rho = data.frame(
  rho_1 = rlkjcorr(n = 1e4, K = 2, eta = 1)[,1,2], 
   rho_2 = rlkjcorr(n = 1e4, K = 2, eta = 2)[,1,2],
   rho_4 = rlkjcorr(n = 1e4, K = 2, eta = 4)[,1,2]
)

rho %>% ggplot()+
  geom_density(aes(rho_1), adjust = .5) +
  geom_density(aes(rho_2), adjust = .5) +
  geom_density(aes(rho_4), adjust = .5) +
  annotate(geom = "text", x = c(0,0,0), y = c(.4, .6, .9), label = c("eta = 1", "eta = 2", "eta = 4"))

```

###Stan
We can start writing the Stan program now. We need some new coding techniques to deal with the matrixs and vectors in our model.

```{r}
m13.1 <- map2stan( 
  alist(
    wait ~ dnorm( mu , sigma ),
    mu <- a_cafe[cafe_id] + b_cafe[cafe_id]*afternoon, 
    c(a_cafe,b_cafe)[cafe_id] ~ dmvnorm2(c(a,b),sigma_cafe,Rho), 
    a ~ dnorm(0,10),
    b ~ dnorm(0,10),
    sigma_cafe ~ dcauchy(0,2),
    sigma ~ dcauchy(0,2),
    Rho ~ dlkjcorr(2)
),
data=d_cafe,
iter=5000 , warmup=2000 , chains=2 )
stancode(m13.1)
```

```{r, results='hide'}
m13.1 = "
data{
	int N;
	int N_cafe;
	int cafe_id[N];
	int afternoon[N];
	real wait[N];
}
parameters{
	
	//parameters
	real alpha[N_cafe];
	real beta[N_cafe];
	real<lower=0> sigma_cafe;

	//hyperparameters
	vector<lower=0>[2] sigma_par; // [sigma_alpha, sigma_beta]
	corr_matrix[2] R;
	real hyper_alpha;
	real hyper_beta;

}
transformed parameters{

	//variable declarations
	vector[2] Mu;					// Mu vector in MVN
	matrix[2, 2] Sigma;				// Sigma matrix in MVN
	vector[2] alpha_beta[N_cafe];	// y vector
	vector[N] mu_cafe;				// mu_cafe


	// Mu vector in MVN
	Mu[1] = hyper_alpha;
	Mu[2] = hyper_beta;
	
	// Sigma matrix in MVN
	Sigma = quad_form_diag(R, sigma_par); 
	// i.e. diag_matrix(sigma_par)  * R * diag_matrix(sigma_par)

	// y vector
	for (j in 1:N_cafe) {
		alpha_beta[j, 1] = alpha[j];
		alpha_beta[j, 2] = beta[j];
	}

	// mu_cafe
	for (i in 1:N){
		mu_cafe[i] = alpha[cafe_id[i]] + beta[cafe_id[i]] * afternoon[i];}

}
model{
	
	//model
	wait ~ normal(mu_cafe, sigma_cafe);

	//prior
	alpha_beta ~ multi_normal(Mu, Sigma);
	sigma_cafe ~ exponential(1);

	//hyper prior
	hyper_alpha ~ normal(5, 2);
	hyper_beta ~ normal(-1, 0.5);
	R ~ lkj_corr(2);
	sigma_par ~ exponential(1);


}
generated quantities {
	vector[N] log_lik;
	for (i in 1:N){
		log_lik[i] = normal_lpdf(wait[i] | mu_cafe[i], sigma_cafe);
	}
}
"
dat13.1 = list(
  N = nrow(d_cafe),
  N_cafe = d_cafe$cafe_id %>% n_unique(),
  cafe_id = d_cafe$cafe_id,
  afternoon = d_cafe$afternoon,
  wait = d_cafe$wait
)
fit13.1 = stan(model_code = m13.1, data = dat13.1, cores = 2, chains = 2, iter = 5000)
```
##Posterior
Let's check if our model is able to estimate the correct parameters we set.

$\alpha = 3.5 \\ \beta=-1 \\ \sigma_{\alpha} = 1 \\ \sigma_{\beta} = 0.5 \\ \rho = -0.7$

```{r}
print(fit13.1, 
      pars = c("hyper_alpha", "hyper_beta", "sigma_par", "R"), 
      probs = c())
```

The results shows that the model learned the negative correlation on $\alpha$ and $\beta$ even though it had only observed wait times in morning and afternoon.

We can also see the shrinkage effect of this adaptive regularization model. Let’s plot the unpooled estimates, posterior mean varying effects and the contours from the inferred prior.

#Case2: more than one intercept in many clusters
To see how to construct a model with more than two varying effects as well as with *more than one type of cluster*, we’ll return to the chimpanzee data again.

In tutorial ch12, we have hyper priors for actors, treatments and blocks respectively. The math model is:
$L_i \sim Binomial(1, p_i) \\ logit(p_i) = \alpha_{actor[i]} + \gamma_{block[i]} + \beta_{treat[i]}$

In this chapter, we can further investigate if there exists interactions among (actor, treatment) or (block, treatment).
The linear model for $logit(p_i)$ still contains an average log-odds for each treatment, $\gamma_{TID[i]}$, an effect for each actor in each treatment $\alpha_{actor[i], TID[i]}$, and finally an effect for each block in each treatment, $\beta_{block[i], TID[i]}$.
This yields a total of 4(treatment) + 7(actor) × 4(treatment) + 6(block) × 4(treatment) = 56 parameters to estimate.

So the new likelihood function will be:
$L_i \sim Binomial(1, p_i) \\ logit(p_i) = \alpha_{actor[i], TID[i]} + \gamma_{TID[i]} + \beta_{block[i], TID[i]}$

For these interaction terms, we will need a different way to assign their priors. Since we want to **pool information** among each pair of (actor, treatment) or (block, treatment). So the parameters must come from a same prior distribution.
We will use the **multivariate gaussian** distribution as our prior.
```{r}
d_chimp = data("chimpanzees")
d_chimp
```


```{r}
library(rethinking) 
data(chimpanzees)
d <- chimpanzees 
d$recipient <- NULL 
d$block_id <- d$block
m13.6 <- map2stan( 
  alist(
    # likeliood
    pulled_left ~ dbinom(1,p),
    # linear models
    logit(p) <- A + (BP + BPC*condition)*prosoc_left,
    A <- a + a_actor[actor] + a_block[block_id],
    BP <- bp + bp_actor[actor] + bp_block[block_id], 
    BPC <- bpc + bpc_actor[actor] + bpc_block[block_id],
    # adaptive priors
    c(a_actor,bp_actor,bpc_actor)[actor] ~ dmvnorm2(0,sigma_actor,Rho_actor),
    c(a_block,bp_block,bpc_block)[block_id] ~ dmvnorm2(0,sigma_block,Rho_block),
    # fixed priors
    c(a,bp,bpc) ~ dnorm(0,1), 
    sigma_actor ~ dcauchy(0,2), 
    sigma_block ~ dcauchy(0,2), 
    Rho_actor ~ dlkjcorr(4), 
    Rho_block ~ dlkjcorr(4)
) , data=d , iter=5000 , warmup=1000 , chains=3 , cores=3 )
```
```{r}
stancode(m13.6)
```

#Case3: intercepts + slopes
```{r}
data(KosterLeckie)
kl_data = list(
  N = nrow(kl_dyads),
  N_households = 25,
  hidA = kl_dyads$hidA,
  hidB = kl_dyads$hidB,
  giftsAB = kl_dyads$giftsAB,
  giftsBA = kl_dyads$giftsBA,
  did = kl_dyads$did
)
m14.4 <- ulam( 
  alist(
    giftsAB ~ poisson( lambdaAB ),
    giftsBA ~ poisson( lambdaBA ),
    log(lambdaAB) <- a + gr[hidA,1] + gr[hidB,2] + d[did,1] , 
    log(lambdaBA) <- a + gr[hidB,1] + gr[hidA,2] + d[did,2] , 
    a ~ normal(0,1),
    ##gr matrix of varying effects
    vector[2]:gr[N_households] ~ multi_normal(0,Rho_gr,sigma_gr), 
    Rho_gr ~ lkj_corr(4),
    sigma_gr ~ exponential(1),
    ## dyad effects
    transpars> matrix[N,2]:d <- compose_noncentered( rep_vector(sigma_d,2) , L_Rho_d , z ), 
    matrix[2,N]:z ~ normal( 0 , 1 ),
    cholesky_factor_corr[2]:L_Rho_d ~ lkj_corr_cholesky( 8 ), 
    sigma_d ~ exponential(1),
    ## compute correlation matrix for dyads
    gq> matrix[2,2]:Rho_d <<- multiply_lower_tri_self_transpose( L_Rho_d )
), data=kl_data , chains=1 , cores=1 , iter=2000 )

stancode(m14.4)
```

#Case4: gaussian process
```{r}
data(islandsDistMatrix)
# display short column names, so fits on screen
Dmat <- islandsDistMatrix
colnames(Dmat) <- c("Ml","Ti","SC","Ya","Fi","Tr","Ch","Mn","To","Ha") 
round(Dmat,1)

data(Kline2) # load the ordinary data, now with coordinates 
d <- Kline2
d$society <- 1:10 # index observations

m13.7 <- map2stan( 
  alist(
    total_tools ~ dpois(lambda),
    log(lambda) <- a + g[society] + bp*logpop, 
    g[society] ~ GPL2( Dmat , etasq , rhosq , 0.01 ), a ~ dnorm(0,10),
    bp ~ dnorm(0,1),
    etasq ~ dcauchy(0,1),
    rhosq ~ dcauchy(0,1)),
  data=
    list(
      total_tools=d$total_tools, 
      logpop=d$logpop, 
      society=d$society, 
      Dmat=islandsDistMatrix),
  warmup=2000 , iter=1e4 , chains=4 )
```


